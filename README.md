# ğŸš€ Automatically Store S3 File Metadata in DynamoDB using AWS Lambda

## ğŸ“˜ Project Summary

This project implements a **serverless, eventâ€‘driven AWS architecture** that **automatically captures metadata** of files uploaded to **Amazon S3** and stores it in **Amazon DynamoDB** using **AWS Lambda**.

The solution is **highly scalable, costâ€‘efficient, and productionâ€‘ready**, and closely follows realâ€‘world enterprise design patterns.

------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

## ğŸ¯ Business Use Case

Organizations require automated visibility into files uploaded to object storage for:

* ğŸ“ Document Management Systems
* ğŸ” File audit and compliance tracking
* ğŸ“Š Metadata indexing and reporting
* ğŸ§© Data ingestion pipelines

This project solves these requirements **without servers**.

------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

## ğŸ—ï¸ Architecture Overview

```
User / Application
        â”‚
        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Amazon S3    â”‚
â”‚ ObjectCreated  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚
        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ AWS Lambda     â”‚
â”‚ Metadata Logic â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚
        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”nâ”‚ Amazon DynamoDBâ”‚
â”‚ Metadata Table â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

## ğŸ”„ Endâ€‘toâ€‘End Flow

* 1ï¸âƒ£ User uploads a file to the **S3 bucket**
* 2ï¸âƒ£ S3 generates an **ObjectCreated event**
* 3ï¸âƒ£ Event **triggers AWS Lambda**
* 4ï¸âƒ£ Lambda extracts object metadata
* 5ï¸âƒ£ Metadata is written to **DynamoDB**
* 6ï¸âƒ£ Execution logs stored in **CloudWatch**

------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

## ğŸ§° AWS Services Used

| Service       | Purpose                     |
| ------------- | --------------------------- |
| ğŸª£ Amazon S3  | File storage & event source |
| âš¡ AWS Lambda  | Event processing logic      |
| ğŸ—„ï¸ DynamoDB  | Metadata persistence        |
| ğŸ” AWS IAM    | Secure access control       |
| ğŸ“Š CloudWatch | Logs & monitoring           |

------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

## ğŸ—‚ï¸ Metadata Captured

| Attribute     | Description             |
| ------------- | ----------------------- |
| `ObjectKey`   | File name (Primary Key) |
| `BucketName`  | Source S3 bucket        |
| `FileSize`    | Size in bytes           |
| `ETag`        | Object checksum         |
| `UploadTime`  | S3 upload timestamp     |
| `ProcessedAt` | Lambda execution time   |

------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

## ğŸ§± DynamoDB Design

**Table Name:** `S3FileMetadata`

**Primary Key:**

* `ObjectKey` (String)

**Billing Mode:**

* Onâ€‘Demand (PAY_PER_REQUEST)

------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

## ğŸ” IAM Role & Security

Lambda execution role permissions:

* Read access to S3 objects
* Write access to DynamoDB
* Logging access to CloudWatch

```json
{
  "Effect": "Allow",
  "Action": ["s3:GetObject"],
  "Resource": "arn:aws:s3:::<bucket-name>/*"
}
```

```json
{
  "Effect": "Allow",
  "Action": ["dynamodb:PutItem"],
  "Resource": "arn:aws:dynamodb:*:*:table/S3FileMetadata"
}
```

---

## âš™ï¸ AWS Lambda Function (Python)

```python
import json
import boto3
from urllib.parse import unquote_plus
from datetime import datetime

s3_client = boto3.client('s3')
dynamodb = boto3.resource('dynamodb')

TABLE_NAME = "S3FilesMetadata"
table = dynamodb.Table(TABLE_NAME)


def lambda_handler(event, context):
    try:
        for record in event['Records']:
            bucket_name = record['s3']['bucket']['name']
            object_key = unquote_plus(record['s3']['object']['key'])

            response = s3_client.head_object(
                Bucket=bucket_name,
                Key=object_key
            )

            item = {
                # MUST MATCH DynamoDB PARTITION KEY NAME
                "FileName": object_key,

                "BucketName": bucket_name,
                "FileSize": response['ContentLength'],
                "ContentType": response.get('ContentType', 'unknown'),
                "LastModified": response['LastModified'].isoformat(),
                "UploadedAt": datetime.utcnow().isoformat()
            }

            table.put_item(Item=item)
            print(f"Metadata stored successfully for {object_key}")

        return {
            "statusCode": 200,
            "body": json.dumps("Metadata stored successfully")
        }

    except Exception as e:
        print("ERROR:", str(e))
        raise

```

------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

## ğŸªœ Stepâ€‘byâ€‘Step Project Implementation Guide

This section provides a **complete, sequential walkthrough** of building the project from scratch, aligned with the architecture shown.

---

### ğŸª£ Step 1: Create an Amazon S3 Bucket

* Create an S3 bucket (example: `s3-file-metadata-bucket`)
* Region: Same as Lambda and DynamoDB
* Enable **Block Public Access** (recommended)
* (Optional) Enable **Versioning** for future audit requirements

**Purpose:** Acts as the file ingestion layer and event source.

------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------


------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
### ğŸ—„ï¸ Step 2: Create DynamoDB Table

* Table Name: `S3FileMetadata`
* Partition Key: `ObjectKey` (String)
* Billing Mode: **Onâ€‘Demand (PAY_PER_REQUEST)**
* Encryption: Enabled by default

**Purpose:** Stores structured metadata for each uploaded object.

------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

### ğŸ” Step 3: Create IAM Role for Lambda

Create an IAM role with the following permissions:

* Read access to S3 objects
* Write access to the DynamoDB table
* Write access to CloudWatch Logs

**Purpose:** Ensures secure, leastâ€‘privilege access between AWS services.

------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

### âš¡ Step 4: Create AWS Lambda Function

* Function Name: `S3MetadataToDynamoDB`
* Runtime: Python 3.x
* Memory: 128 MB
* Timeout: 30 seconds
* Execution Role: IAM role created in Step 3

**Purpose:** Processes S3 events and extracts metadata.

------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

### ğŸ§  Step 5: Add Lambda Function Code

* Paste the provided Python code into the Lambda function
* Update the DynamoDB table name if required
* Save and deploy the function

**Purpose:** Implements metadata extraction and persistence logic.

------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

### ğŸ”” Step 6: Configure S3 Event Notification

* Navigate to S3 bucket â†’ Properties â†’ Event notifications
* Event type: **ObjectCreated (All)**
* Destination: AWS Lambda
* Select the Lambda function created earlier
* (Optional) Add suffix filters like `.pdf`, `.jpg`

**Purpose:** Automatically triggers Lambda on file uploads.

------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

### ğŸ“¤ Step 7: Upload File to S3 (Testing)

* Upload any file (PDF, image, text) to the S3 bucket
* Ensure upload completes successfully

**Purpose:** Validates endâ€‘toâ€‘end event flow.

------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

### ğŸ“Š Step 8: Verify Lambda Execution

* Open Amazon CloudWatch Logs
* Locate the Lambda log group
* Confirm successful execution without errors

**Purpose:** Ensures Lambda is triggered and runs correctly.

------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

### âœ… Step 9: Verify Metadata in DynamoDB

* Open DynamoDB table
* View table items
* Confirm metadata fields are populated correctly

**Purpose:** Confirms successful metadata persistence.

------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

### ğŸ“ˆ Step 10: Monitoring and Validation

* Monitor Lambda invocation metrics
* Check DynamoDB write capacity usage
* Enable CloudWatch alarms if required

**Purpose:** Ensures reliability and operational visibility.

------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

## ğŸ§ª Validation & Testing

* Upload any file to S3
* Confirm Lambda invocation in CloudWatch
* Validate item insertion in DynamoDB

------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

## ğŸ“Š Monitoring & Logging

* CloudWatch Logs â€“ Lambda execution
* CloudWatch Metrics â€“ Errors & duration
* DynamoDB Metrics â€“ Write usage

------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

## ğŸ’° Cost Efficiency

* âœ” Fully serverless
* âœ” Payâ€‘perâ€‘use model
* âœ” No idle infrastructure cost




